{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall adapter-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelWithHeads\n",
    "\n",
    "# model = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "# adapter_name = model.load_adapter(\"AdapterHub/bert-base-uncased-pf-conll2003\", source=\"hf\")\n",
    "# model.active_adapters = adapter_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "# example = \"My name is Wolfgang and I live in Berlin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanfangwuyu/opt/anaconda3/envs/Classifier/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:135: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99267524,\n",
       "  'word': ' Philipp',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99980456,\n",
       "  'word': ' Germany',\n",
       "  'start': 31,\n",
       "  'end': 38}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### USEFUL MODEL 1\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"philschmid/distilroberta-base-ner-conll2003\")\n",
    "model1 = AutoModelForTokenClassification.from_pretrained(\"philschmid/distilroberta-base-ner-conll2003\")\n",
    "\n",
    "model1_nlp = pipeline(\"ner\", model=model1, tokenizer=tokenizer, grouped_entities=True)\n",
    "example = \"My name is Philipp and live in Germany\"\n",
    "\n",
    "model1_nlp(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# # os.chdir('InstructionNER')\n",
    "# from instruction_ner.model import Model\n",
    "# model = Model(\n",
    "#     model_path_or_name=\"olgaduchovny/t5-base-ner-mit-movie\",\n",
    "#     tokenizer_path_or_name=\"olgaduchovny/t5-base-ner-mit-movie\"\n",
    "# )\n",
    "# options = [\n",
    "#         \"ACTOR\",\n",
    "#         \"AWARD\",\n",
    "#         \"CHARACTER\",\n",
    "#         \"DIRECTOR\",\n",
    "#         \"GENRE\",\n",
    "#         \"OPINION\",\n",
    "#         \"ORIGIN\",\n",
    "#         \"PLOT\",\n",
    "#         \"QUOTE\",\n",
    "#         \"RELATIONSHIP\",\n",
    "#         \"SOUNDTRACK\",\n",
    "#         \"YEAR\"\n",
    "#     ]\n",
    "# instruction = \"please extract entities and their types from the input sentence, \" \\\n",
    "#               \"all entity types are in options\"\n",
    "# text = \"are there any good romantic comedies out right now\"\n",
    "# generation_kwargs = {\n",
    "#     \"num_beams\": 2,\n",
    "#     \"max_length\": 128\n",
    "# }\n",
    "# pred_spans = model.predict(\n",
    "#     text=text,\n",
    "#     generation_kwargs=generation_kwargs,\n",
    "#     instruction=instruction,\n",
    "#     options=options\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('good is a REVIEW, romantic comedies is a GENRE.', [(19, 36, 'GENRE')])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.predict(\n",
    "#     text=text,\n",
    "#     generation_kwargs=generation_kwargs,\n",
    "#     instruction=instruction,\n",
    "#     options=options\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanfangwuyu/opt/anaconda3/envs/Classifier/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "### USEFUL MODEL 2\n",
    "\n",
    "from transformers import pipeline\n",
    "model2_nlp = pipeline('ner', model='dbmdz/bert-large-cased-finetuned-conll03-english')\n",
    "message = 'Show me a picture of Halle Berry.'\n",
    "entities = model2_nlp(message, aggregation_strategy=\"simple\")\n",
    "# name = ''\n",
    "# for entity in entities:\n",
    "#     name = name + entity['word']\n",
    "#     # print(f\"{entity['word']}: {entity['entity_group']} ({entity['score']:.2f})\")\n",
    "# name = name.replace('#','')\n",
    "# print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sent Sim\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sent_sim = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9990139, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': 0.999645, 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "# from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model3 = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "model3_nlp = pipeline(\"ner\", model=model3, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = model3_nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_origin = []\n",
    "Q_ents = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_origin.extend([\n",
    "    \"Who is the director of Good Will Hunting?\",\n",
    "    \"Who directed The Bridge on the River Kwai?\",\n",
    "    \"Who is the director of Star Wars: Episode VI - Return of the Jedi?\",\n",
    "])\n",
    "\n",
    "Q_ents.extend([\n",
    "    \"Good Will Hunting\",\n",
    "    \"The Bridge on the River Kwai\",\n",
    "    \"Star Wars: Episode VI - Return of the Jedi\",\n",
    "]) # should be extract by model\n",
    "\n",
    "qs_origin.extend([\n",
    "    \"Who is the screenwriter of The Masked Gang: Cyprus?\",\n",
    "    \"What is the MPAA film rating of Weathering with You?\",\n",
    "    \"What is the genre of Good Neighbors?\",\n",
    "])\n",
    "\n",
    "Q_ents.extend([\n",
    "    \"The Masked Gang: Cyprus\",\n",
    "    \"Weathering with You\",\n",
    "    \"Good Neighbors\",\n",
    "])\n",
    "\n",
    "qs_origin.extend([\n",
    "    \"What is the box office of The Princess and the Frog?\",\n",
    "    \"Can you tell me the publication date of Tom Meets Zizou?\",\n",
    "    \"Who is the executive producer of X-Men: First Class?\",\n",
    "])\n",
    "\n",
    "Q_ents.extend([\n",
    "    \"The Princess and the Frog\",\n",
    "    \"Tom Meets Zizou\",\n",
    "    \"X-Men: First Class\",\n",
    "])\n",
    "\n",
    "qs_origin.extend([\n",
    "    \"Show me a picture of Halle Berry.\",\n",
    "    \"What does Julia Roberts look like?\",\n",
    "    \"Let me know what Sandra Bullock looks like.\",\n",
    "])\n",
    "\n",
    "Q_ents.extend([\n",
    "    \"Halle Berry\",\n",
    "    \"Julia Roberts\",\n",
    "    \"Sandra Bullock\",\n",
    "])# TODO if many ents a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Name_Qwiki_Qid = pd.read_csv('name_Qwiki_Qid.tsv', sep='\\t')\n",
    "Name_Qwiki_Qid[-5:]\n",
    "\n",
    "names = Name_Qwiki_Qid['Str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_embeddings = sent_sim.encode(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(name_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"name_embeddings.npy\", name_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Ground truth: Good Will Hunting\n",
      "Predicted name entity:  ['Good Will Hunting']\n",
      "--------------------------------------------\n",
      "Ground truth: The Bridge on the River Kwai\n",
      "Predicted name entity:  ['The Bridge', 'River Kwai']\n",
      "--------------------------------------------\n",
      "Ground truth: Star Wars: Episode VI - Return of the Jedi\n",
      "Predicted name entity:  ['Star Wars: Episode VI', 'Return of the Jedi']\n",
      "--------------------------------------------\n",
      "Ground truth: The Masked Gang: Cyprus\n",
      "Predicted name entity:  ['The', 'Masked Gang:', 'Cyprus']\n",
      "--------------------------------------------\n",
      "Ground truth: Weathering with You\n",
      "Predicted name entity:  ['MP', 'AA', 'Weather', 'ing', 'with You']\n",
      "--------------------------------------------\n",
      "Ground truth: Good Neighbors\n",
      "Predicted name entity:  ['Good Neighbors']\n",
      "--------------------------------------------\n",
      "Ground truth: The Princess and the Frog\n",
      "Predicted name entity:  ['The Princess and the Frog']\n",
      "--------------------------------------------\n",
      "Ground truth: Tom Meets Zizou\n",
      "Predicted name entity:  ['Tom Meets Zizou']\n",
      "--------------------------------------------\n",
      "Ground truth: X-Men: First Class\n",
      "Predicted name entity:  ['X', '-', 'Men: First Class']\n",
      "--------------------------------------------\n",
      "Ground truth: Halle Berry\n",
      "Predicted name entity:  ['H', 'alle Berry']\n",
      "--------------------------------------------\n",
      "Ground truth: Julia Roberts\n",
      "Predicted name entity:  ['Julia Roberts']\n",
      "--------------------------------------------\n",
      "Ground truth: Sandra Bullock\n",
      "Predicted name entity:  ['Sandra Bullock']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "acc = 0\n",
    "for i, q in enumerate(qs_origin):\n",
    "    preds = model1_nlp(q, aggregation_strategy=\"simple\")\n",
    "    ents = []\n",
    "    for pred in preds:\n",
    "        ent = pred['word']\n",
    "        if ent[0] == ' ':\n",
    "            ent = ent[1:]\n",
    "        ents.append(ent)\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Ground truth: {}\".format(Q_ents[i]))\n",
    "    print(\"Predicted name entity: \", ents)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Ground truth: Good Will Hunting\n",
      "Predicted name entity:  ['Good Will Hunting']\n",
      "--------------------------------------------\n",
      "Ground truth: The Bridge on the River Kwai\n",
      "Predicted name entity:  ['The Bridge on the River Kwai']\n",
      "--------------------------------------------\n",
      "Ground truth: Star Wars: Episode VI - Return of the Jedi\n",
      "Predicted name entity:  ['Star Wars', 'Episode VI', 'Return of the Jedi']\n",
      "--------------------------------------------\n",
      "Ground truth: The Masked Gang: Cyprus\n",
      "Predicted name entity:  ['The Masked Gang : Cyprus']\n",
      "--------------------------------------------\n",
      "Ground truth: Weathering with You\n",
      "Predicted name entity:  ['MPAA', 'Weathering with You']\n",
      "--------------------------------------------\n",
      "Ground truth: Good Neighbors\n",
      "Predicted name entity:  ['Good Nei', '##bors']\n",
      "--------------------------------------------\n",
      "Ground truth: The Princess and the Frog\n",
      "Predicted name entity:  ['The Princess and the Frog']\n",
      "--------------------------------------------\n",
      "Ground truth: Tom Meets Zizou\n",
      "Predicted name entity:  ['Tom Meets', 'Z', '##izou']\n",
      "--------------------------------------------\n",
      "Ground truth: X-Men: First Class\n",
      "Predicted name entity:  ['X - Men : First Class']\n",
      "--------------------------------------------\n",
      "Ground truth: Halle Berry\n",
      "Predicted name entity:  ['Halle Berry']\n",
      "--------------------------------------------\n",
      "Ground truth: Julia Roberts\n",
      "Predicted name entity:  ['Julia Roberts']\n",
      "--------------------------------------------\n",
      "Ground truth: Sandra Bullock\n",
      "Predicted name entity:  ['Sandra Bullock']\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "all_ents = []\n",
    "for i, q in enumerate(qs_origin):\n",
    "    preds = model2_nlp(q, aggregation_strategy=\"simple\")\n",
    "    ents = []\n",
    "    for pred in preds:\n",
    "        ent = pred['word']\n",
    "        if ent[0] == ' ':\n",
    "            ent = ent[1:]\n",
    "        ents.append(ent)\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Ground truth: {}\".format(Q_ents[i]))\n",
    "    print(\"Predicted name entity: \", ents)\n",
    "    all_ents.append(ents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ents = [' '.join(ents) for ents in all_ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try sent_sim for preds and whole sents compared to true Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2_embeddings = sent_sim.encode(all_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  Good Will Hunting\n",
      "y:  Good Will Hunting\n"
     ]
    }
   ],
   "source": [
    "\n",
    "matchings = []\n",
    "for i, embd in enumerate(pred2_embeddings[:1]):\n",
    "    results = [util.pytorch_cos_sim(embd, name_embd) for name_embd in name_embeddings]\n",
    "    matching = names[results.index(max(results))]\n",
    "    matchings.append(matching)\n",
    "    print(\"X: \", all_ents[i])\n",
    "    print(\"y: \", matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"mps\")\n",
    "# torch.nn.functional.cosine_similarity(torch.tensor(pred2_embeddings[0]), torch.tensor(name_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here with GPU is even slower\n",
    "# util.pytorch_cos_sim(torch.tensor(pred2_embeddings, device=device), torch.tensor(name_embeddings, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = util.pytorch_cos_sim(torch.tensor(pred2_embeddings), torch.tensor(name_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.max(sims, 1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  Good Will Hunting\n",
      "y:  Good Will Hunting\n",
      "X:  The Bridge on the River Kwai\n",
      "y:  The Bridge on the River Kwai\n",
      "X:  Star Wars Episode VI Return of the Jedi\n",
      "y:  Star Wars Episode VI: Return of the Jedi\n",
      "X:  The Masked Gang : Cyprus\n",
      "y:  The Masked Gang: Cyprus\n",
      "X:  MPAA Weathering with You\n",
      "y:  Weathering with You\n",
      "X:  Good Nei ##bors\n",
      "y:  Bor\n",
      "X:  The Princess and the Frog\n",
      "y:  The Princess and the Frog\n",
      "X:  Tom Meets Z ##izou\n",
      "y:  Tom Meets Zizou\n",
      "X:  X - Men : First Class\n",
      "y:  X-Men: First Class\n",
      "X:  Halle Berry\n",
      "y:  Halle Berry\n",
      "X:  Julia Roberts\n",
      "y:  Julia Roberts\n",
      "X:  Sandra Bullock\n",
      "y:  Sandra Bullock\n"
     ]
    }
   ],
   "source": [
    "for i, id in enumerate(idx):\n",
    "    print(\"X: \", all_ents[i])\n",
    "    print(\"y: \", names[int(id)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try with start-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 40\n",
      "Ground truth: Good Will Hunting\n",
      "Predicted name entity: Good Will Hunting\n",
      "--------------------------------------------\n",
      "13 41\n",
      "Ground truth: The Bridge on the River Kwai\n",
      "Predicted name entity: The Bridge on the River Kwai\n",
      "--------------------------------------------\n",
      "23 65\n",
      "Ground truth: Star Wars: Episode VI - Return of the Jedi\n",
      "Predicted name entity: Star Wars: Episode VI - Return of the Jedi\n",
      "--------------------------------------------\n",
      "27 50\n",
      "Ground truth: The Masked Gang: Cyprus\n",
      "Predicted name entity: The Masked Gang: Cyprus\n",
      "--------------------------------------------\n",
      "12 51\n",
      "Ground truth: Weathering with You\n",
      "Predicted name entity: MPAA film rating of Weathering with You\n",
      "--------------------------------------------\n",
      "21 35\n",
      "Ground truth: Good Neighbors\n",
      "Predicted name entity: Good Neighbors\n",
      "--------------------------------------------\n",
      "26 51\n",
      "Ground truth: The Princess and the Frog\n",
      "Predicted name entity: The Princess and the Frog\n",
      "--------------------------------------------\n",
      "40 55\n",
      "Ground truth: Tom Meets Zizou\n",
      "Predicted name entity: Tom Meets Zizou\n",
      "--------------------------------------------\n",
      "33 51\n",
      "Ground truth: X-Men: First Class\n",
      "Predicted name entity: X-Men: First Class\n",
      "--------------------------------------------\n",
      "21 32\n",
      "Ground truth: Halle Berry\n",
      "Predicted name entity: Halle Berry\n",
      "--------------------------------------------\n",
      "10 23\n",
      "Ground truth: Julia Roberts\n",
      "Predicted name entity: Julia Roberts\n",
      "--------------------------------------------\n",
      "17 31\n",
      "Ground truth: Sandra Bullock\n",
      "Predicted name entity: Sandra Bullock\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "all_ents = []\n",
    "for i, q in enumerate(qs_origin):\n",
    "    preds = model2_nlp(q, aggregation_strategy=\"simple\")\n",
    "\n",
    "    start = preds[0]\n",
    "    end = preds[-1]\n",
    "    start_id = start['start']\n",
    "    end_id = end['end']\n",
    "    print(start_id, end_id)\n",
    "    all_ent = q[start_id: end_id]\n",
    "        \n",
    "        \n",
    "    \n",
    "    print(\"Ground truth: {}\".format(Q_ents[i]))\n",
    "    print(\"Predicted name entity: {}\".format(all_ent))\n",
    "    print(\"--------------------------------------------\")\n",
    "    all_ents.append(all_ent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  Good Will Hunting\n",
      "y:  Good Will Hunting\n",
      "X:  The Bridge on the River Kwai\n",
      "y:  The Bridge on the River Kwai\n",
      "X:  Star Wars: Episode VI - Return of the Jedi\n",
      "y:  Star Wars: Episode VI – Return of the Jedi\n",
      "X:  The Masked Gang: Cyprus\n",
      "y:  The Masked Gang: Cyprus\n",
      "X:  MPAA film rating of Weathering with You\n",
      "y:  Weathering with You\n",
      "X:  Good Neighbors\n",
      "y:  Good Neighbors\n",
      "X:  The Princess and the Frog\n",
      "y:  The Princess and the Frog\n",
      "X:  Tom Meets Zizou\n",
      "y:  Tom Meets Zizou\n",
      "X:  X-Men: First Class\n",
      "y:  X-Men: First Class\n",
      "X:  Halle Berry\n",
      "y:  Halle Berry\n",
      "X:  Julia Roberts\n",
      "y:  Julia Roberts\n",
      "X:  Sandra Bullock\n",
      "y:  Sandra Bullock\n"
     ]
    }
   ],
   "source": [
    "all_ents_embeddings = sent_sim.encode(all_ents)\n",
    "sims = util.pytorch_cos_sim(torch.tensor(all_ents_embeddings), torch.tensor(name_embeddings))\n",
    "idx = torch.max(sims, 1).indices\n",
    "for i, id in enumerate(idx):\n",
    "    print(\"X: \", all_ents[i])\n",
    "    print(\"y: \", names[int(id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_app():\n",
    "    pass\n",
    "\n",
    "def ner_test(X, y, names=names,   name_embeddings=name_embeddings, model_ner=model2_nlp, model_sent_sim=sent_sim, strategy='simple'):\n",
    "\n",
    "    all_ents = []\n",
    "    for i, q in enumerate(X):\n",
    "        preds = model_ner(q, aggregation_strategy=strategy)\n",
    "        if len(preds) == 0:\n",
    "            print(\"---Try model 3---\")\n",
    "            preds = model3_nlp(q, aggregation_strategy=strategy)\n",
    "            if len(preds) == 0:\n",
    "                print(q)\n",
    "                print('No name entity was found.')\n",
    "                all_ents.append(q)\n",
    "            else:\n",
    "                all_ent = q[preds[0]['start']: preds[-1]['end']]\n",
    "                all_ents.append(all_ent)\n",
    "        else:\n",
    "            all_ent = q[preds[0]['start']: preds[-1]['end']]\n",
    "            # remove substring 'xxx rating'\n",
    "            all_ents.append(all_ent)\n",
    "\n",
    "    all_ents_embeddings = model_sent_sim.encode(all_ents)\n",
    "    sims = util.pytorch_cos_sim(torch.tensor(all_ents_embeddings), torch.tensor(name_embeddings))\n",
    "    idx = torch.max(sims, 1).indices\n",
    "    \n",
    "    ner_pred = all_ents\n",
    "    y_pred = np.array([names[int(id)] for id in idx])\n",
    "    y = np.array(y)\n",
    "    # for i in range(len(y)):\n",
    "    #     if y_pred[i]!=y[i]:\n",
    "    #         # print(\"-----------------------------\")\n",
    "    #         print(\"Question: {}\".format(X[i]))\n",
    "    #         print(\"NER prediction: {}\".format(ner_pred[i]))\n",
    "    #         print(\"NER matching: {}\".format(y_pred[i]))\n",
    "    #         print(\"Ground truth: {}\".format(y[i]))\n",
    "    #         print(\"-----------------------------\")\n",
    "    #         # print(X[i], ner_pred[i], y_pred[i], y[i], )\n",
    "    accuracy = np.mean(y==y_pred)\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = pd.read_csv('Datasets/Relations_small_X_y.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, step = 0, 10000\n",
    "ner_test(list(small_data['X'][i:i+step]), list(small_data['y'][i:i+step]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_for_recommend(X, y, names=names,   name_embeddings=name_embeddings, model_ner=model2_nlp, model_sent_sim=sent_sim, strategy='simple'):\n",
    "    X = [\n",
    "        # \"Recommend movies like Nightmare on Elm Street, Friday the 13th, and Halloween.\",\n",
    "        # \"Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?\",\n",
    "        \"Recommend movies similar to Hamlet and Othello.\",\n",
    "    ]\n",
    "\n",
    "    all_ents = []\n",
    "    for i, q in enumerate(X):\n",
    "        preds = model_ner(q, aggregation_strategy=strategy)\n",
    "        ents = [pred['word'] for pred in preds ]\n",
    "        all_ents.append(ents)\n",
    "    \n",
    "        ents_embeddings = model_sent_sim.encode(ents)\n",
    "        sims = util.pytorch_cos_sim(torch.tensor(ents_embeddings), torch.tensor(name_embeddings))\n",
    "        idx = torch.max(sims, 1).indices\n",
    "        print(idx)\n",
    "        for id in np.array(idx):\n",
    "            print(names[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ner_for_recommend' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ner_for_recommend([],[])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ner_for_recommend' is not defined"
     ]
    }
   ],
   "source": [
    "ner_for_recommend([],[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try with whole sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_question_embeddings = sent_sim.encode(qs_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Str      Star Wars Episode VI: Return of the Jedi\n",
       "Wiki    <http://www.wikidata.org/entity/Q3795592>\n",
       "Qid                                      Q3795592\n",
       "Name: 95043, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Name_Qwiki_Qid.iloc[list(names).index(\"Star Wars Episode VI: Return of the Jedi\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  Who is the director of Good Will Hunting?\n",
      "y:  Good Will Hunting\n",
      "X:  Who directed The Bridge on the River Kwai?\n",
      "y:  River Kwai bridge\n",
      "X:  Who is the director of Star Wars: Episode VI - Return of the Jedi?\n",
      "y:  Star Wars: Episode VI – Return of the Jedi\n",
      "X:  Who is the screenwriter of The Masked Gang: Cyprus?\n",
      "y:  The Masked Gang: Cyprus\n",
      "X:  What is the MPAA film rating of Weathering with You?\n",
      "y:  Filmiroda rating category\n",
      "X:  What is the genre of Good Neighbors?\n",
      "y:  Good Neighbors\n",
      "X:  What is the box office of The Princess and the Frog?\n",
      "y:  The Princess and the Frog\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [86], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m matchings \u001b[39m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i, embd \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(whole_question_embeddings):\n\u001b[0;32m----> 3\u001b[0m     results \u001b[39m=\u001b[39m [util\u001b[39m.\u001b[39mpytorch_cos_sim(embd, name_embd) \u001b[39mfor\u001b[39;00m name_embd \u001b[39min\u001b[39;00m name_embeddings]\n\u001b[1;32m      4\u001b[0m     matching \u001b[39m=\u001b[39m names[results\u001b[39m.\u001b[39mindex(\u001b[39mmax\u001b[39m(results))]\n\u001b[1;32m      5\u001b[0m     matchings\u001b[39m.\u001b[39mappend(matching)\n",
      "Cell \u001b[0;32mIn [86], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m matchings \u001b[39m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i, embd \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(whole_question_embeddings):\n\u001b[0;32m----> 3\u001b[0m     results \u001b[39m=\u001b[39m [util\u001b[39m.\u001b[39;49mpytorch_cos_sim(embd, name_embd) \u001b[39mfor\u001b[39;00m name_embd \u001b[39min\u001b[39;00m name_embeddings]\n\u001b[1;32m      4\u001b[0m     matching \u001b[39m=\u001b[39m names[results\u001b[39m.\u001b[39mindex(\u001b[39mmax\u001b[39m(results))]\n\u001b[1;32m      5\u001b[0m     matchings\u001b[39m.\u001b[39mappend(matching)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Classifier/lib/python3.9/site-packages/sentence_transformers/util.py:28\u001b[0m, in \u001b[0;36mpytorch_cos_sim\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpytorch_cos_sim\u001b[39m(a: Tensor, b: Tensor):\n\u001b[1;32m     24\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[39mreturn\u001b[39;00m cos_sim(a, b)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Classifier/lib/python3.9/site-packages/sentence_transformers/util.py:48\u001b[0m, in \u001b[0;36mcos_sim\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     45\u001b[0m     b \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     47\u001b[0m a_norm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mnormalize(a, p\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m b_norm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mnormalize(b, p\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmm(a_norm, b_norm\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Classifier/lib/python3.9/site-packages/torch/nn/functional.py:4632\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(input, p, dim, eps, out)\u001b[0m\n\u001b[1;32m   4630\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(normalize, (\u001b[39minput\u001b[39m, out), \u001b[39minput\u001b[39m, p\u001b[39m=\u001b[39mp, dim\u001b[39m=\u001b[39mdim, eps\u001b[39m=\u001b[39meps, out\u001b[39m=\u001b[39mout)\n\u001b[1;32m   4631\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4632\u001b[0m     denom \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(p, dim, keepdim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mclamp_min(eps)\u001b[39m.\u001b[39mexpand_as(\u001b[39minput\u001b[39m)\n\u001b[1;32m   4633\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m \u001b[39m/\u001b[39m denom\n\u001b[1;32m   4634\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# matchings = []\n",
    "# for i, embd in enumerate(whole_question_embeddings):\n",
    "#     results = [util.pytorch_cos_sim(embd, name_embd) for name_embd in name_embeddings]\n",
    "#     matching = names[results.index(max(results))]\n",
    "#     matchings.append(matching)\n",
    "#     print(\"X: \", qs_origin[i])\n",
    "#     print(\"y: \", matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = 0\n",
    "# for i, q in enumerate(qs_origin):\n",
    "#     pred = nlp(q)\n",
    "#     ent = pred[0]['word']\n",
    "#     if ent[0] == ' ':\n",
    "#         ent = ent[1:]\n",
    "#     print(\"({})\".format(ent), \"({})\".format(Q_ents[i]))\n",
    "#     print(ent==Q_ents[i])\n",
    "#     acc += (ent == Q_ents[i])\n",
    "# print(acc/len(qs_origin))\n",
    "\n",
    "# acc = 0\n",
    "# for i, q in enumerate(qs_origin):\n",
    "#     pred = model.predict(\n",
    "#         text=q,\n",
    "#         generation_kwargs=generation_kwargs,\n",
    "#         instruction=instruction,\n",
    "#         options=options,\n",
    "#     )\n",
    "#     print(pred[1])\n",
    "#     ent = q(pred[1][0][0], pred[1][0][1])\n",
    "#     print(\"({})\".format(ent), \"({})\".format(Q_ents[i]))\n",
    "#     print(ent==Q_ents[i])\n",
    "#     acc += (ent == Q_ents[i])\n",
    "# print(acc/len(qs_origin))\n",
    "\n",
    "\n",
    "acc = 0\n",
    "for i, q in enumerate(qs_origin):\n",
    "    pred = model2_nlp(q, aggregation_strategy=\"simple\")\n",
    "    print(pred[0]['word'])\n",
    "    ent = pred[0]['word']\n",
    "    print(\"({})\".format(ent), \"({})\".format(Q_ents[i]))\n",
    "    print(ent==Q_ents[i])\n",
    "    acc += (ent == Q_ents[i])\n",
    "print(acc/len(qs_origin))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Ground truth: Good Will Hunting\n",
      "Predicted name entity:  ['Good', 'Will Hunting'] ['MISC', 'ORG']\n",
      "--------------------------------------------\n",
      "Ground truth: The Bridge on the River Kwai\n",
      "Predicted name entity:  ['The Bridge on the River K', '##wai'] ['MISC', 'LOC']\n",
      "--------------------------------------------\n",
      "Ground truth: Star Wars: Episode VI - Return of the Jedi\n",
      "Predicted name entity:  ['Star Wars', 'Episode VI', 'Return of the Jedi'] ['MISC', 'MISC', 'MISC']\n",
      "--------------------------------------------\n",
      "Ground truth: The Masked Gang: Cyprus\n",
      "Predicted name entity:  ['The Masked Gang', 'Cyprus'] ['MISC', 'LOC']\n",
      "--------------------------------------------\n",
      "Ground truth: Weathering with You\n",
      "Predicted name entity:  ['MPAA', 'Weathering with You?'] ['ORG', 'MISC']\n",
      "--------------------------------------------\n",
      "Ground truth: Good Neighbors\n",
      "Predicted name entity:  ['Good Neighbors'] ['MISC']\n",
      "--------------------------------------------\n",
      "Ground truth: The Princess and the Frog\n",
      "Predicted name entity:  ['The Princess and the Frog'] ['MISC']\n",
      "--------------------------------------------\n",
      "Ground truth: Tom Meets Zizou\n",
      "Predicted name entity:  ['Tom Meets Zizou'] ['PER']\n",
      "--------------------------------------------\n",
      "Ground truth: X-Men: First Class\n",
      "Predicted name entity:  ['X - Men :', 'Class'] ['MISC', 'MISC']\n",
      "--------------------------------------------\n",
      "Ground truth: Halle Berry\n",
      "Predicted name entity:  ['Halle Berry'] ['PER']\n",
      "--------------------------------------------\n",
      "Ground truth: Julia Roberts\n",
      "Predicted name entity:  ['Julia Roberts'] ['PER']\n",
      "--------------------------------------------\n",
      "Ground truth: Sandra Bullock\n",
      "Predicted name entity:  ['Sandra Bullock'] ['PER']\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "all_ents = []\n",
    "for i, q in enumerate(qs_origin):\n",
    "    preds = model3_nlp(q, aggregation_strategy='simple')\n",
    "    ents = []\n",
    "    tags = []\n",
    "    # if len(preds) == 0:\n",
    "    #     return None\n",
    "    for pred in preds:\n",
    "        ent = pred['word']\n",
    "        # tag = pred['entity_group']\n",
    "        if ent[0] == ' ':\n",
    "            ent = ent[1:]\n",
    "        ents.append(ent)\n",
    "        # tags.append(tag)\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Ground truth: {}\".format(Q_ents[i]))\n",
    "    print(\"Predicted name entity: \", ents)\n",
    "    all_ents.append(ents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "几个方向\n",
    "1. 找最好的ner 模型\n",
    "2. 看看除了策略还有没有可调参数 比如消除##\n",
    "3. 自己train/fine-tune： 需要冷启动训练集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy Test (Not useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = qs_origin[0]\n",
    "choices = list(names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('Classifier')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d5ef6a9993a551c539aca5214fca9cb4a5b4a6263da4c15a05317b840935140"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
